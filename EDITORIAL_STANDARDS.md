# The District: Data Integrity Constitution

**This is a permanent reference document. Every article must comply with these principles.**

---

## I. THE CARDINAL RULES

### 1. NO CLAIMS WITHOUT DATA
- NEVER write a statistic that isn't in the DATA object
- NEVER extrapolate beyond what the data shows
- NEVER invent numbers to make a story sound better
- If a claim requires data that doesn't exist, DON'T MAKE THE CLAIM

### 2. NO TRENDS WITHOUT TIME SERIES
- "X increased 12% year-over-year" requires actual YoY data
- Growth in data collection ≠ growth in the real world
- More recent data ≠ trend
- Acknowledge when apparent "trends" reflect data collection changes

### 3. SAMPLE SIZE ALWAYS MATTERS
- "100% approval" means nothing without n=X
- Small samples (n<20) should NEVER be presented as definitive
- Always show: "City X (n=Y meetings/mentions): Z%"
- Add explicit caveats for small samples

### 4. AVERAGES CAN LIE
- "49.9 average sentiment" on a 0-100 scale is literally just "neutral by definition"
- Don't present midpoint values as findings
- Focus on distributions, outliers, and meaningful variance
- Ask: "Is this average actually telling us something, or is it noise?"

---

## II. STATISTICAL FALLACIES TO NEVER COMMIT

| Fallacy | Example of Violation | How to Avoid |
|---------|---------------------|--------------|
| **Data Collection Artifact** | "12x surge in 2 years" when really our database expanded | Always ask: "Did reality change, or did our measurement change?" |
| **Selection Bias** | "Cities are YIMBY" when we only study cities with active proposals | Acknowledge: "These cities self-selected by having X" |
| **Meaningless Averages** | "49.9 sentiment = divided nation" | Focus on distribution, not central tendency |
| **Arbitrary Cutoffs** | "≥55 = YIMBY, ≤45 = NIMBY" | Acknowledge: "These thresholds are analytical conventions, not bright lines" |
| **Ecological Fallacy** | State averages applied to individual cities | Don't generalize aggregate patterns to specific cases |
| **Survivorship Bias** | Only counting projects that reached a vote | Acknowledge what we CAN'T see |
| **Confusing Discussion with Decision** | "High sentiment = will approve" | Sentiment measures tone, not outcomes |
| **Cherry-Picking** | Highlighting one quote that supports our narrative | Show the full picture |
| **Fabrication** | Making up "3x more opposition in residential zones" with no data | EVERY claim needs a DATA object source |

---

## III. SOURCE VERIFICATION REQUIREMENTS

### Before Publishing Any Claim:

1. **Is the claim in the DATA object?**
   - If NO → DELETE THE CLAIM

2. **Can the data source be verified?**
   - Hamlet transcripts: ✅ (our primary source)
   - Census data: ✅ (link to census.gov)
   - News articles: ✅ (must be from Tier 1/2 sources)
   - "Common knowledge": ❌ VERIFY OR REMOVE

3. **Do the sources actually support the claims?**
   - Sources must validate the SPECIFIC claims made
   - A source about "Princeton housing debate" doesn't validate "96% approval rate"

### Source Tiers:
- **Tier 1**: Government records, official meeting minutes, Census Bureau
- **Tier 2**: Major news outlets (NYT, WSJ, local papers of record)
- **Tier 3**: Industry publications, local blogs (use with caution)
- **Never**: Social media, unverified claims, "I heard that..."

---

## IV. METHODOLOGY REQUIREMENTS

Every article methodology section MUST include:

1. **Data Source**: Where did this data come from?
2. **Sample Size**: How many X did we analyze?
3. **Date Range**: What time period does this cover?
4. **Selection Criteria**: Why these cities/officials/meetings and not others?
5. **Definitions**: What do our metrics actually measure? (e.g., "adversarial language includes...")
6. **Limitations**: What can this analysis NOT tell us?
7. **Weighting/Formula Justification**: If we use a formula, why those weights?

---

## V. PRE-PUBLICATION CHECKLIST

Before any article goes live, verify:

- [ ] Every statistic traces to the DATA object
- [ ] No claims without data backing
- [ ] No timeline claims that could be data collection artifacts
- [ ] Sample sizes shown for key metrics
- [ ] Methodology explains definitions and limitations
- [ ] Sources actually support the claims made (not just related topics)
- [ ] No future-dated sources
- [ ] Hardcoded numbers match computed values
- [ ] Averages are meaningful, not just midpoints
- [ ] Selection bias acknowledged where present

---

## VI. WHEN IN DOUBT

**Ask these questions:**

1. "Would a skeptical statistician tear this apart?" → If yes, fix it
2. "What would a journalist's fact-checker find?" → Verify first
3. "Am I making the data say what I want, or what it actually shows?" → Let data lead
4. "Could this expose us to legal liability?" → When uncertain, caveat heavily or remove
5. "Is this interesting because it's true, or because I want it to be true?" → Kill your darlings

---

## VII. THE STANDARD

**The District exists to illuminate local government through data.**

We are not advocates. We are not activists. We are journalists who let the data speak.

If the data doesn't support a compelling story, we don't manufacture one. We find a different angle that the data DOES support.

Credibility is our only asset. Once lost, it cannot be recovered.

---

*Applies to all articles in The District*
